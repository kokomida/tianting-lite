<execution>
  <constraint>
    ## å‘é‡æœç´¢é™åˆ¶æ¡ä»¶
    - **æ€§èƒ½çº¦æŸ**ï¼šæœç´¢å“åº”æ—¶é—´å¿…é¡»åœ¨æ¯«ç§’çº§åˆ«
    - **å†…å­˜çº¦æŸ**ï¼šå‘é‡ç´¢å¼•çš„å†…å­˜ä½¿ç”¨æœ‰é™åˆ¶
    - **ç²¾åº¦çº¦æŸ**ï¼šæœç´¢ç»“æœçš„ç›¸å…³æ€§å¿…é¡»è¾¾åˆ°è¦æ±‚
    - **å¹¶å‘çº¦æŸ**ï¼šç³»ç»Ÿå¿…é¡»æ”¯æŒé«˜å¹¶å‘æœç´¢è¯·æ±‚
    - **æ‰©å±•æ€§çº¦æŸ**ï¼šç´¢å¼•ç»“æ„å¿…é¡»æ”¯æŒåŠ¨æ€æ‰©å±•
  </constraint>

  <rule>
    ## å‘é‡æœç´¢å¼ºåˆ¶è§„åˆ™
    - **ç´¢å¼•ä¸€è‡´æ€§**ï¼šå‘é‡ç´¢å¼•å¿…é¡»ä¸åŸå§‹æ•°æ®ä¿æŒä¸€è‡´
    - **æœç´¢å‡†ç¡®æ€§**ï¼šç›¸ä¼¼åº¦è®¡ç®—å¿…é¡»å‡†ç¡®å¯é 
    - **å®æ—¶æ›´æ–°**ï¼šæ–°å¢å‘é‡å¿…é¡»åŠæ—¶åŠ å…¥ç´¢å¼•
    - **ç»“æœæ’åº**ï¼šæœç´¢ç»“æœå¿…é¡»æŒ‰ç›¸ä¼¼åº¦æ­£ç¡®æ’åº
    - **é”™è¯¯å¤„ç†**ï¼šæœç´¢å¤±è´¥å¿…é¡»æœ‰æ˜ç¡®çš„é”™è¯¯ä¿¡æ¯
  </rule>

  <guideline>
    ## å‘é‡æœç´¢æŒ‡å¯¼åŸåˆ™
    - **æ•ˆç‡ä¼˜å…ˆ**ï¼šä¼˜å…ˆé€‰æ‹©é«˜æ•ˆçš„æœç´¢ç®—æ³•
    - **ç²¾åº¦å¹³è¡¡**ï¼šåœ¨é€Ÿåº¦å’Œç²¾åº¦é—´æ‰¾åˆ°æœ€ä½³å¹³è¡¡
    - **èµ„æºä¼˜åŒ–**ï¼šåˆç†ä½¿ç”¨è®¡ç®—å’Œå­˜å‚¨èµ„æº
    - **ç”¨æˆ·ä½“éªŒ**ï¼šæä¾›æµç•…çš„æœç´¢ä½“éªŒ
    - **å¯æ‰©å±•æ€§**ï¼šè®¾è®¡æ”¯æŒæœªæ¥æ‰©å±•çš„æ¶æ„
  </guideline>

  <process>
    ## ğŸ” å‘é‡æœç´¢å·¥ä½œæµç¨‹

    ### æœç´¢ç³»ç»Ÿæ¶æ„
    ```mermaid
    graph TD
        A[æœç´¢è¯·æ±‚] --> B[æŸ¥è¯¢é¢„å¤„ç†]
        B --> C[å‘é‡ç¼–ç ]
        C --> D[ç´¢å¼•æ£€ç´¢]
        D --> E[ç›¸ä¼¼åº¦è®¡ç®—]
        E --> F[ç»“æœæ’åº]
        F --> G[åå¤„ç†]
        G --> H[ç»“æœè¿”å›]
        
        D --> D1[HNSWç´¢å¼•]
        D --> D2[IVFç´¢å¼•]
        D --> D3[LSHç´¢å¼•]
        
        E --> E1[ä½™å¼¦ç›¸ä¼¼åº¦]
        E --> E2[æ¬§å‡ é‡Œå¾—è·ç¦»]
        E --> E3[å†…ç§¯ç›¸ä¼¼åº¦]
        
        G --> G1[ç»“æœè¿‡æ»¤]
        G --> G2[é‡æ’åº]
        G --> G3[å…ƒæ•°æ®è¡¥å……]
    ```

    ### ç¬¬ä¸€é˜¶æ®µï¼šæŸ¥è¯¢é¢„å¤„ç†
    ```mermaid
    flowchart TD
        A[åŸå§‹æŸ¥è¯¢] --> B[æ–‡æœ¬æ¸…æ´—]
        B --> C[åˆ†è¯å¤„ç†]
        C --> D[åœç”¨è¯è¿‡æ»¤]
        D --> E[æŸ¥è¯¢æ‰©å±•]
        E --> F[æŸ¥è¯¢å‘é‡åŒ–]
        
        B --> B1[å»é™¤ç‰¹æ®Šå­—ç¬¦]
        B --> B2[ç»Ÿä¸€ç¼–ç æ ¼å¼]
        
        C --> C1[ä¸­æ–‡åˆ†è¯]
        C --> C2[è‹±æ–‡åˆ†è¯]
        
        E --> E1[åŒä¹‰è¯æ‰©å±•]
        E --> E2[ç›¸å…³è¯æ‰©å±•]
        
        F --> F1[Embeddingç”Ÿæˆ]
        F --> F2[å‘é‡æ ‡å‡†åŒ–]
    ```

    ### ç¬¬äºŒé˜¶æ®µï¼šå‘é‡æ£€ç´¢
    ```mermaid
    graph TD
        A[æŸ¥è¯¢å‘é‡] --> B[ç´¢å¼•é€‰æ‹©]
        B --> C[ç²—æ’æ£€ç´¢]
        C --> D[ç²¾æ’è®¡ç®—]
        D --> E[TopKé€‰æ‹©]
        E --> F[ç»“æœèšåˆ]
        
        B --> B1[HNSWå›¾ç´¢å¼•]
        B --> B2[IVFèšç±»ç´¢å¼•]
        B --> B3[PQé‡åŒ–ç´¢å¼•]
        
        C --> C1[å€™é€‰é›†ç”Ÿæˆ]
        C --> C2[è·ç¦»ä¼°ç®—]
        
        D --> D1[ç²¾ç¡®è·ç¦»è®¡ç®—]
        D --> D2[ç›¸ä¼¼åº¦è¯„åˆ†]
        
        E --> E1[å †æ’åº]
        E --> E2[é˜ˆå€¼è¿‡æ»¤]
    ```

    ### ç¬¬ä¸‰é˜¶æ®µï¼šç»“æœä¼˜åŒ–
    ```mermaid
    flowchart LR
        A[åˆå§‹ç»“æœ] --> B[ç›¸å…³æ€§è¯„ä¼°]
        B --> C[å¤šæ ·æ€§ä¼˜åŒ–]
        C --> D[ä¸ªæ€§åŒ–è°ƒæ•´]
        D --> E[ç»“æœé‡æ’]
        E --> F[æœ€ç»ˆè¾“å‡º]
        
        B --> B1[è¯­ä¹‰ç›¸å…³æ€§]
        B --> B2[ä¸»é¢˜ç›¸å…³æ€§]
        
        C --> C1[å»é‡å¤„ç†]
        C --> C2[å¤šæ ·æ€§æ³¨å…¥]
        
        D --> D1[ç”¨æˆ·åå¥½]
        D --> D2[å†å²è¡Œä¸º]
        
        E --> E1[å­¦ä¹ æ’åº]
        E --> E2[è§„åˆ™è°ƒæ•´]
    ```

    ## ğŸ› ï¸ æŠ€æœ¯å®ç°æ¡†æ¶

    ### Pythonå‘é‡æœç´¢å®ç°
    ```python
    import numpy as np
    import faiss
    from sentence_transformers import SentenceTransformer
    import hnswlib
    
    class VectorSearchEngine:
        def __init__(self, dimension=384, index_type='hnsw'):
            self.dimension = dimension
            self.index_type = index_type
            self.encoder = SentenceTransformer('all-MiniLM-L6-v2')
            self.index = None
            self.documents = []
            self.init_index()
        
        def init_index(self):
            """åˆå§‹åŒ–å‘é‡ç´¢å¼•"""
            if self.index_type == 'hnsw':
                self.index = hnswlib.Index(space='cosine', dim=self.dimension)
                self.index.init_index(max_elements=1000000, ef_construction=200, M=16)
            elif self.index_type == 'faiss':
                self.index = faiss.IndexFlatIP(self.dimension)
            elif self.index_type == 'ivf':
                quantizer = faiss.IndexFlatIP(self.dimension)
                self.index = faiss.IndexIVFFlat(quantizer, self.dimension, 100)
        
        def add_documents(self, documents):
            """æ·»åŠ æ–‡æ¡£åˆ°ç´¢å¼•"""
            # ç”Ÿæˆå‘é‡
            vectors = self.encoder.encode(documents)
            vectors = vectors / np.linalg.norm(vectors, axis=1, keepdims=True)
            
            # æ·»åŠ åˆ°ç´¢å¼•
            if self.index_type == 'hnsw':
                start_id = len(self.documents)
                ids = np.arange(start_id, start_id + len(documents))
                self.index.add_items(vectors, ids)
            else:
                self.index.add(vectors.astype('float32'))
            
            # ä¿å­˜æ–‡æ¡£
            self.documents.extend(documents)
        
        def search(self, query, top_k=10, ef=50):
            """æœç´¢ç›¸ä¼¼æ–‡æ¡£"""
            # æŸ¥è¯¢å‘é‡åŒ–
            query_vector = self.encoder.encode([query])
            query_vector = query_vector / np.linalg.norm(query_vector)
            
            # æ‰§è¡Œæœç´¢
            if self.index_type == 'hnsw':
                self.index.set_ef(ef)
                labels, distances = self.index.knn_query(query_vector, k=top_k)
                results = []
                for i, (label, distance) in enumerate(zip(labels[0], distances[0])):
                    results.append({
                        'document': self.documents[label],
                        'score': 1 - distance,  # è½¬æ¢ä¸ºç›¸ä¼¼åº¦
                        'rank': i + 1
                    })
            else:
                distances, indices = self.index.search(query_vector.astype('float32'), top_k)
                results = []
                for i, (idx, distance) in enumerate(zip(indices[0], distances[0])):
                    results.append({
                        'document': self.documents[idx],
                        'score': distance,
                        'rank': i + 1
                    })
            
            return results
        
        def update_document(self, doc_id, new_content):
            """æ›´æ–°æ–‡æ¡£"""
            if doc_id < len(self.documents):
                self.documents[doc_id] = new_content
                # é‡æ–°ç”Ÿæˆå‘é‡å¹¶æ›´æ–°ç´¢å¼•
                new_vector = self.encoder.encode([new_content])
                new_vector = new_vector / np.linalg.norm(new_vector)
                
                if self.index_type == 'hnsw':
                    # HNSWéœ€è¦é‡å»ºç´¢å¼•æ¥æ›´æ–°
                    self.rebuild_index()
                else:
                    # Faisså¯ä»¥ç›´æ¥æ›¿æ¢
                    self.index.reconstruct(doc_id, new_vector.astype('float32'))
        
        def rebuild_index(self):
            """é‡å»ºç´¢å¼•"""
            if self.documents:
                self.init_index()
                self.add_documents(self.documents)
    ```

    ### é«˜çº§æœç´¢åŠŸèƒ½
    ```python
    class AdvancedVectorSearch(VectorSearchEngine):
        def __init__(self, *args, **kwargs):
            super().__init__(*args, **kwargs)
            self.metadata = []
        
        def add_documents_with_metadata(self, documents, metadata):
            """æ·»åŠ å¸¦å…ƒæ•°æ®çš„æ–‡æ¡£"""
            self.add_documents(documents)
            self.metadata.extend(metadata)
        
        def filtered_search(self, query, filters=None, top_k=10):
            """å¸¦è¿‡æ»¤æ¡ä»¶çš„æœç´¢"""
            # å…ˆè¿›è¡Œå‘é‡æœç´¢
            initial_results = self.search(query, top_k * 3)  # è·å–æ›´å¤šå€™é€‰
            
            # åº”ç”¨è¿‡æ»¤æ¡ä»¶
            filtered_results = []
            for result in initial_results:
                doc_idx = self.documents.index(result['document'])
                metadata = self.metadata[doc_idx] if doc_idx < len(self.metadata) else {}
                
                # æ£€æŸ¥è¿‡æ»¤æ¡ä»¶
                if self._match_filters(metadata, filters):
                    result['metadata'] = metadata
                    filtered_results.append(result)
                
                if len(filtered_results) >= top_k:
                    break
            
            return filtered_results
        
        def _match_filters(self, metadata, filters):
            """æ£€æŸ¥å…ƒæ•°æ®æ˜¯å¦åŒ¹é…è¿‡æ»¤æ¡ä»¶"""
            if not filters:
                return True
            
            for key, value in filters.items():
                if key not in metadata:
                    return False
                if isinstance(value, list):
                    if metadata[key] not in value:
                        return False
                else:
                    if metadata[key] != value:
                        return False
            
            return True
        
        def hybrid_search(self, query, text_weight=0.7, vector_weight=0.3, top_k=10):
            """æ··åˆæœç´¢ï¼ˆæ–‡æœ¬+å‘é‡ï¼‰"""
            # å‘é‡æœç´¢
            vector_results = self.search(query, top_k * 2)
            
            # æ–‡æœ¬æœç´¢ï¼ˆç®€å•çš„å…³é”®è¯åŒ¹é…ï¼‰
            text_results = self._text_search(query, top_k * 2)
            
            # ç»“æœèåˆ
            combined_scores = {}
            
            # å‘é‡æœç´¢ç»“æœ
            for result in vector_results:
                doc = result['document']
                combined_scores[doc] = vector_weight * result['score']
            
            # æ–‡æœ¬æœç´¢ç»“æœ
            for result in text_results:
                doc = result['document']
                if doc in combined_scores:
                    combined_scores[doc] += text_weight * result['score']
                else:
                    combined_scores[doc] = text_weight * result['score']
            
            # æ’åºå¹¶è¿”å›
            sorted_results = sorted(combined_scores.items(), 
                                  key=lambda x: x[1], reverse=True)
            
            return [{'document': doc, 'score': score, 'rank': i+1} 
                   for i, (doc, score) in enumerate(sorted_results[:top_k])]
        
        def _text_search(self, query, top_k):
            """ç®€å•çš„æ–‡æœ¬æœç´¢"""
            query_terms = query.lower().split()
            scores = []
            
            for doc in self.documents:
                doc_lower = doc.lower()
                score = sum(1 for term in query_terms if term in doc_lower)
                score = score / len(query_terms) if query_terms else 0
                scores.append({'document': doc, 'score': score})
            
            scores.sort(key=lambda x: x['score'], reverse=True)
            return scores[:top_k]
    ```

    ## ğŸ“Š æœç´¢æ€§èƒ½ä¼˜åŒ–

    ### ç´¢å¼•ä¼˜åŒ–ç­–ç•¥
    ```mermaid
    graph TD
        A[ç´¢å¼•ä¼˜åŒ–] --> B[ç»“æ„ä¼˜åŒ–]
        A --> C[å‚æ•°è°ƒä¼˜]
        A --> D[å†…å­˜ä¼˜åŒ–]
        A --> E[å¹¶å‘ä¼˜åŒ–]
        
        B --> B1[HNSWå›¾ç»“æ„]
        B --> B2[IVFèšç±»æ•°é‡]
        B --> B3[PQé‡åŒ–å‚æ•°]
        
        C --> C1[ef_construction]
        C --> C2[Mè¿æ¥æ•°]
        C --> C3[efæœç´¢å‚æ•°]
        
        D --> D1[å‘é‡å‹ç¼©]
        D --> D2[æ‰¹é‡åŠ è½½]
        D --> D3[å†…å­˜æ˜ å°„]
        
        E --> E1[è¯»å†™åˆ†ç¦»]
        E --> E2[æŸ¥è¯¢ç¼“å­˜]
        E --> E3[è´Ÿè½½å‡è¡¡]
    ```

    ### æœç´¢è´¨é‡è¯„ä¼°
    ```python
    def evaluate_search_quality(search_engine, test_queries, ground_truth):
        """è¯„ä¼°æœç´¢è´¨é‡"""
        metrics = {
            'precision_at_k': [],
            'recall_at_k': [],
            'ndcg_at_k': [],
            'mrr': []
        }
        
        for query, relevant_docs in zip(test_queries, ground_truth):
            results = search_engine.search(query, top_k=10)
            retrieved_docs = [r['document'] for r in results]
            
            # è®¡ç®—Precision@K
            precision = len(set(retrieved_docs) & set(relevant_docs)) / len(retrieved_docs)
            metrics['precision_at_k'].append(precision)
            
            # è®¡ç®—Recall@K
            recall = len(set(retrieved_docs) & set(relevant_docs)) / len(relevant_docs)
            metrics['recall_at_k'].append(recall)
            
            # è®¡ç®—NDCG@K
            ndcg = calculate_ndcg(retrieved_docs, relevant_docs)
            metrics['ndcg_at_k'].append(ndcg)
            
            # è®¡ç®—MRR
            mrr = calculate_mrr(retrieved_docs, relevant_docs)
            metrics['mrr'].append(mrr)
        
        # è®¡ç®—å¹³å‡å€¼
        for key in metrics:
            metrics[key] = np.mean(metrics[key])
        
        return metrics
    ```

    ## ğŸ”„ åŠ¨æ€ç´¢å¼•ç®¡ç†

    ### å¢é‡æ›´æ–°æœºåˆ¶
    ```mermaid
    flowchart LR
        A[æ–°æ–‡æ¡£] --> B[å‘é‡ç”Ÿæˆ]
        B --> C[ç´¢å¼•æ›´æ–°]
        C --> D[ä¸€è‡´æ€§æ£€æŸ¥]
        D --> E{æ›´æ–°æˆåŠŸ?}
        E -->|æ˜¯| F[ç´¢å¼•ä¼˜åŒ–]
        E -->|å¦| G[å›æ»šæ“ä½œ]
        
        C --> C1[åœ¨çº¿æ›´æ–°]
        C --> C2[æ‰¹é‡æ›´æ–°]
        
        F --> F1[ç´¢å¼•å‹ç¼©]
        F --> F2[æ€§èƒ½è°ƒä¼˜]
        
        G --> G1[é”™è¯¯æ—¥å¿—]
        G --> G2[é‡è¯•æœºåˆ¶]
    ```

    ### ç´¢å¼•ç»´æŠ¤ç­–ç•¥
    ```python
    def maintain_index(search_engine, maintenance_config):
        """ç´¢å¼•ç»´æŠ¤"""
        # å®šæœŸé‡å»ºç´¢å¼•
        if maintenance_config.get('rebuild_threshold', 0.1):
            fragmentation = calculate_index_fragmentation(search_engine)
            if fragmentation > maintenance_config['rebuild_threshold']:
                search_engine.rebuild_index()
        
        # æ¸…ç†è¿‡æœŸæ–‡æ¡£
        if maintenance_config.get('cleanup_enabled', False):
            expired_docs = find_expired_documents(search_engine)
            for doc_id in expired_docs:
                search_engine.remove_document(doc_id)
        
        # æ€§èƒ½ç›‘æ§
        performance_metrics = collect_performance_metrics(search_engine)
        if performance_metrics['avg_search_time'] > maintenance_config.get('max_search_time', 100):
            optimize_search_parameters(search_engine)
    ```
  </process>

  <criteria>
    ## å‘é‡æœç´¢è¯„ä»·æ ‡å‡†

    ### æœç´¢æ€§èƒ½
    - âœ… æœç´¢å“åº”æ—¶é—´ â‰¤ 50ms
    - âœ… å¹¶å‘æŸ¥è¯¢æ”¯æŒ â‰¥ 1000 QPS
    - âœ… ç´¢å¼•æ„å»ºæ—¶é—´åˆç†
    - âœ… å†…å­˜ä½¿ç”¨æ•ˆç‡ â‰¥ 80%

    ### æœç´¢è´¨é‡
    - âœ… Top-10å‡†ç¡®ç‡ â‰¥ 85%
    - âœ… å¬å›ç‡ â‰¥ 90%
    - âœ… NDCG@10 â‰¥ 0.8
    - âœ… å¹³å‡å€’æ•°æ’å â‰¥ 0.75

    ### ç³»ç»Ÿå¯é æ€§
    - âœ… ç´¢å¼•ä¸€è‡´æ€§ â‰¥ 99.9%
    - âœ… ç³»ç»Ÿå¯ç”¨æ€§ â‰¥ 99.5%
    - âœ… æ•°æ®å®Œæ•´æ€§ 100%
    - âœ… é”™è¯¯æ¢å¤èƒ½åŠ›å¼º

    ### å¯æ‰©å±•æ€§
    - âœ… æ”¯æŒå‘é‡æ•°é‡ â‰¥ 10M
    - âœ… æ°´å¹³æ‰©å±•èƒ½åŠ›è‰¯å¥½
    - âœ… åŠ¨æ€æ›´æ–°æ”¯æŒ
    - âœ… å­˜å‚¨æ•ˆç‡ä¼˜åŒ–
  </criteria>
</execution>
