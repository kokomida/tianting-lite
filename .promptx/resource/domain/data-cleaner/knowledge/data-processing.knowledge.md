<knowledge>
  ## ðŸ”„ æ•°æ®å¤„ç†æ ¸å¿ƒçŸ¥è¯†

  ### æ•°æ®å¤„ç†ç”Ÿå‘½å‘¨æœŸ
  - **æ•°æ®é‡‡é›†**ï¼šä»Žå„ç§æ•°æ®æºèŽ·å–åŽŸå§‹æ•°æ®
  - **æ•°æ®å­˜å‚¨**ï¼šå°†æ•°æ®å­˜å‚¨åœ¨åˆé€‚çš„å­˜å‚¨ç³»ç»Ÿä¸­
  - **æ•°æ®æ¸…æ´—**ï¼šæ¸…ç†å’Œé¢„å¤„ç†æ•°æ®ï¼Œæé«˜æ•°æ®è´¨é‡
  - **æ•°æ®è½¬æ¢**ï¼šå°†æ•°æ®è½¬æ¢ä¸ºé€‚åˆåˆ†æžçš„æ ¼å¼
  - **æ•°æ®åˆ†æž**ï¼šä»Žæ•°æ®ä¸­æå–æœ‰ä»·å€¼çš„ä¿¡æ¯å’Œæ´žå¯Ÿ
  - **æ•°æ®å¯è§†åŒ–**ï¼šä»¥å›¾è¡¨å½¢å¼å±•ç¤ºåˆ†æžç»“æžœ
  - **æ•°æ®å½’æ¡£**ï¼šé•¿æœŸä¿å­˜é‡è¦çš„åŽ†å²æ•°æ®

  ### æ•°æ®ç±»åž‹ä¸Žç‰¹å¾
  ```mermaid
  graph TD
      A[æ•°æ®ç±»åž‹] --> B[ç»“æž„åŒ–æ•°æ®]
      A --> C[åŠç»“æž„åŒ–æ•°æ®]
      A --> D[éžç»“æž„åŒ–æ•°æ®]
      
      B --> B1[å…³ç³»æ•°æ®åº“]
      B --> B2[CSVæ–‡ä»¶]
      B --> B3[Excelè¡¨æ ¼]
      
      C --> C1[JSONæ•°æ®]
      C --> C2[XMLæ•°æ®]
      C --> C3[æ—¥å¿—æ–‡ä»¶]
      
      D --> D1[æ–‡æœ¬æ–‡æ¡£]
      D --> D2[å›¾åƒæ•°æ®]
      D --> D3[éŸ³é¢‘è§†é¢‘]
  ```

  ### æ•°æ®è´¨é‡ç»´åº¦
  - **å®Œæ•´æ€§ï¼ˆCompletenessï¼‰**ï¼šæ•°æ®æ˜¯å¦å®Œæ•´ï¼Œæ— ç¼ºå¤±å€¼
  - **å‡†ç¡®æ€§ï¼ˆAccuracyï¼‰**ï¼šæ•°æ®æ˜¯å¦æ­£ç¡®åæ˜ çŽ°å®ž
  - **ä¸€è‡´æ€§ï¼ˆConsistencyï¼‰**ï¼šæ•°æ®åœ¨ä¸åŒåœ°æ–¹æ˜¯å¦ä¸€è‡´
  - **æ—¶æ•ˆæ€§ï¼ˆTimelinessï¼‰**ï¼šæ•°æ®æ˜¯å¦åŠæ—¶æ›´æ–°
  - **æœ‰æ•ˆæ€§ï¼ˆValidityï¼‰**ï¼šæ•°æ®æ˜¯å¦ç¬¦åˆå®šä¹‰çš„æ ¼å¼å’Œè§„åˆ™
  - **å”¯ä¸€æ€§ï¼ˆUniquenessï¼‰**ï¼šæ•°æ®æ˜¯å¦å­˜åœ¨é‡å¤

  ## ðŸ“Š æ•°æ®æ¸…æ´—æŠ€æœ¯

  ### ç¼ºå¤±å€¼å¤„ç†æ–¹æ³•
  ```mermaid
  mindmap
    root((ç¼ºå¤±å€¼å¤„ç†))
      åˆ é™¤æ–¹æ³•
        åˆ—è¡¨åˆ é™¤
        æˆå¯¹åˆ é™¤
        é˜ˆå€¼åˆ é™¤
      å¡«å……æ–¹æ³•
        å‡å€¼å¡«å……
        ä¸­ä½æ•°å¡«å……
        ä¼—æ•°å¡«å……
        å‰å‘å¡«å……
        åŽå‘å¡«å……
      é¢„æµ‹æ–¹æ³•
        çº¿æ€§æ’å€¼
        å¤šé¡¹å¼æ’å€¼
        æœºå™¨å­¦ä¹ é¢„æµ‹
        æ—¶é—´åºåˆ—é¢„æµ‹
  ```

  ### å¼‚å¸¸å€¼æ£€æµ‹ä¸Žå¤„ç†
  - **ç»Ÿè®¡æ–¹æ³•**ï¼šZ-Scoreã€IQRã€Grubbsæ£€éªŒ
  - **å¯è§†åŒ–æ–¹æ³•**ï¼šç®±çº¿å›¾ã€æ•£ç‚¹å›¾ã€ç›´æ–¹å›¾
  - **æœºå™¨å­¦ä¹ æ–¹æ³•**ï¼šIsolation Forestã€One-Class SVMã€LOF
  - **å¤„ç†ç­–ç•¥**ï¼šåˆ é™¤ã€æ›¿æ¢ã€å˜æ¢ã€æ ‡è®°

  ### é‡å¤æ•°æ®å¤„ç†
  ```python
  # é‡å¤æ•°æ®æ£€æµ‹ç¤ºä¾‹
  import pandas as pd
  
  # å®Œå…¨é‡å¤
  duplicates = df.duplicated()
  
  # åŸºäºŽç‰¹å®šåˆ—çš„é‡å¤
  duplicates = df.duplicated(subset=['name', 'email'])
  
  # æ¨¡ç³ŠåŒ¹é…é‡å¤
  from fuzzywuzzy import fuzz
  
  def fuzzy_dedupe(df, column, threshold=80):
      """åŸºäºŽæ¨¡ç³ŠåŒ¹é…çš„åŽ»é‡"""
      to_remove = []
      for i in range(len(df)):
          for j in range(i+1, len(df)):
              similarity = fuzz.ratio(df.iloc[i][column], df.iloc[j][column])
              if similarity > threshold:
                  to_remove.append(j)
      
      return df.drop(df.index[to_remove])
  ```

  ## ðŸ”§ æ•°æ®è½¬æ¢æŠ€æœ¯

  ### æ•°æ®ç±»åž‹è½¬æ¢
  ```python
  # å¸¸è§æ•°æ®ç±»åž‹è½¬æ¢
  import pandas as pd
  import numpy as np
  
  # å­—ç¬¦ä¸²è½¬æ•°å€¼
  df['numeric_column'] = pd.to_numeric(df['string_column'], errors='coerce')
  
  # æ—¥æœŸæ—¶é—´è½¬æ¢
  df['datetime_column'] = pd.to_datetime(df['date_string'], format='%Y-%m-%d')
  
  # åˆ†ç±»æ•°æ®ç¼–ç 
  from sklearn.preprocessing import LabelEncoder, OneHotEncoder
  
  # æ ‡ç­¾ç¼–ç 
  le = LabelEncoder()
  df['category_encoded'] = le.fit_transform(df['category'])
  
  # ç‹¬çƒ­ç¼–ç 
  df_encoded = pd.get_dummies(df, columns=['category'])
  ```

  ### æ•°æ®æ ‡å‡†åŒ–ä¸Žå½’ä¸€åŒ–
  ```python
  from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler
  
  # æ ‡å‡†åŒ– (Z-score)
  scaler = StandardScaler()
  df_standardized = scaler.fit_transform(df[numeric_columns])
  
  # å½’ä¸€åŒ– (Min-Max)
  scaler = MinMaxScaler()
  df_normalized = scaler.fit_transform(df[numeric_columns])
  
  # é²æ£’ç¼©æ”¾
  scaler = RobustScaler()
  df_robust = scaler.fit_transform(df[numeric_columns])
  ```

  ### ç‰¹å¾å·¥ç¨‹
  ```mermaid
  graph LR
      A[åŽŸå§‹ç‰¹å¾] --> B[ç‰¹å¾é€‰æ‹©]
      A --> C[ç‰¹å¾æž„é€ ]
      A --> D[ç‰¹å¾å˜æ¢]
      
      B --> B1[è¿‡æ»¤æ³•]
      B --> B2[åŒ…è£…æ³•]
      B --> B3[åµŒå…¥æ³•]
      
      C --> C1[ç»„åˆç‰¹å¾]
      C --> C2[äº¤äº’ç‰¹å¾]
      C --> C3[èšåˆç‰¹å¾]
      
      D --> D1[å¤šé¡¹å¼ç‰¹å¾]
      D --> D2[å¯¹æ•°å˜æ¢]
      D --> D3[Box-Coxå˜æ¢]
  ```

  ## ðŸ› ï¸ æ•°æ®å¤„ç†å·¥å…·

  ### Pythonæ•°æ®å¤„ç†ç”Ÿæ€
  ```mermaid
  graph TD
      A[Pythonæ•°æ®å¤„ç†] --> B[æ ¸å¿ƒåº“]
      A --> C[ä¸“ä¸šåº“]
      A --> D[å¯è§†åŒ–åº“]
      
      B --> B1[Pandas - æ•°æ®æ“ä½œ]
      B --> B2[NumPy - æ•°å€¼è®¡ç®—]
      B --> B3[SciPy - ç§‘å­¦è®¡ç®—]
      
      C --> C1[Scikit-learn - æœºå™¨å­¦ä¹ ]
      C --> C2[Dask - å¤§æ•°æ®å¤„ç†]
      C --> C3[Modin - å¹¶è¡ŒPandas]
      
      D --> D1[Matplotlib - åŸºç¡€ç»˜å›¾]
      D --> D2[Seaborn - ç»Ÿè®¡ç»˜å›¾]
      D --> D3[Plotly - äº¤äº’ç»˜å›¾]
  ```

  ### å¤§æ•°æ®å¤„ç†æ¡†æž¶
  - **Apache Spark**ï¼šåˆ†å¸ƒå¼æ•°æ®å¤„ç†å¼•æ“Ž
  - **Apache Flink**ï¼šæµå¤„ç†æ¡†æž¶
  - **Apache Kafka**ï¼šåˆ†å¸ƒå¼æµå¹³å°
  - **Hadoop**ï¼šåˆ†å¸ƒå¼å­˜å‚¨å’Œè®¡ç®—æ¡†æž¶
  - **Dask**ï¼šPythonå¹¶è¡Œè®¡ç®—åº“

  ### æ•°æ®è´¨é‡å·¥å…·
  ```python
  # Great Expectations - æ•°æ®éªŒè¯
  import great_expectations as ge
  
  # åˆ›å»ºæ•°æ®æœŸæœ›
  df_ge = ge.from_pandas(df)
  df_ge.expect_column_values_to_not_be_null('important_column')
  df_ge.expect_column_values_to_be_between('age', min_value=0, max_value=120)
  
  # Pandas Profiling - æ•°æ®æ¦‚è§ˆ
  from pandas_profiling import ProfileReport
  
  profile = ProfileReport(df, title="Data Quality Report")
  profile.to_file("data_quality_report.html")
  
  # PyJanitor - æ•°æ®æ¸…æ´—
  import janitor
  
  df_clean = (df
              .clean_names()  # æ¸…ç†åˆ—å
              .remove_empty()  # åˆ é™¤ç©ºè¡Œç©ºåˆ—
              .dropna(subset=['important_column'])  # åˆ é™¤ç‰¹å®šåˆ—çš„ç©ºå€¼
              )
  ```

  ## ðŸ“ˆ æ•°æ®å¤„ç†æœ€ä½³å®žè·µ

  ### æ•°æ®å¤„ç†æµæ°´çº¿è®¾è®¡
  ```python
  from sklearn.pipeline import Pipeline
  from sklearn.compose import ColumnTransformer
  from sklearn.preprocessing import StandardScaler, OneHotEncoder
  from sklearn.impute import SimpleImputer
  
  # æ•°å€¼ç‰¹å¾å¤„ç†æµæ°´çº¿
  numeric_pipeline = Pipeline([
      ('imputer', SimpleImputer(strategy='median')),
      ('scaler', StandardScaler())
  ])
  
  # åˆ†ç±»ç‰¹å¾å¤„ç†æµæ°´çº¿
  categorical_pipeline = Pipeline([
      ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),
      ('encoder', OneHotEncoder(handle_unknown='ignore'))
  ])
  
  # ç»„åˆå¤„ç†æµæ°´çº¿
  preprocessor = ColumnTransformer([
      ('num', numeric_pipeline, numeric_features),
      ('cat', categorical_pipeline, categorical_features)
  ])
  ```

  ### æ•°æ®ç‰ˆæœ¬æŽ§åˆ¶
  - **DVC (Data Version Control)**ï¼šæ•°æ®ç‰ˆæœ¬ç®¡ç†å·¥å…·
  - **Git LFS**ï¼šå¤§æ–‡ä»¶ç‰ˆæœ¬æŽ§åˆ¶
  - **MLflow**ï¼šæœºå™¨å­¦ä¹ ç”Ÿå‘½å‘¨æœŸç®¡ç†
  - **æ•°æ®è¡€ç¼˜è¿½è¸ª**ï¼šè®°å½•æ•°æ®çš„æ¥æºå’Œå˜æ¢åŽ†å²

  ### æ•°æ®å®‰å…¨ä¸Žéšç§
  ```python
  # æ•°æ®è„±æ•ç¤ºä¾‹
  import hashlib
  
  def anonymize_data(df, sensitive_columns):
      """æ•°æ®è„±æ•å¤„ç†"""
      df_anonymized = df.copy()
      
      for column in sensitive_columns:
          # å“ˆå¸Œè„±æ•
          df_anonymized[column] = df_anonymized[column].apply(
              lambda x: hashlib.sha256(str(x).encode()).hexdigest()[:10]
          )
      
      return df_anonymized
  
  # å·®åˆ†éšç§
  def add_noise(data, epsilon=1.0):
      """æ·»åŠ æ‹‰æ™®æ‹‰æ–¯å™ªå£°å®žçŽ°å·®åˆ†éšç§"""
      sensitivity = 1.0  # æ ¹æ®å…·ä½“æƒ…å†µè°ƒæ•´
      scale = sensitivity / epsilon
      noise = np.random.laplace(0, scale, data.shape)
      return data + noise
  ```

  ## ðŸŽ¯ æ€§èƒ½ä¼˜åŒ–ç­–ç•¥

  ### å†…å­˜ä¼˜åŒ–
  ```python
  # æ•°æ®ç±»åž‹ä¼˜åŒ–
  def optimize_dtypes(df):
      """ä¼˜åŒ–æ•°æ®ç±»åž‹ä»¥èŠ‚çœå†…å­˜"""
      for col in df.columns:
          col_type = df[col].dtype
          
          if col_type != 'object':
              c_min = df[col].min()
              c_max = df[col].max()
              
              if str(col_type)[:3] == 'int':
                  if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:
                      df[col] = df[col].astype(np.int8)
                  elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:
                      df[col] = df[col].astype(np.int16)
                  elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:
                      df[col] = df[col].astype(np.int32)
              
              elif str(col_type)[:5] == 'float':
                  if c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:
                      df[col] = df[col].astype(np.float32)
      
      return df
  
  # åˆ†å—å¤„ç†å¤§æ–‡ä»¶
  def process_large_file(filename, chunk_size=10000):
      """åˆ†å—å¤„ç†å¤§æ–‡ä»¶"""
      results = []
      
      for chunk in pd.read_csv(filename, chunksize=chunk_size):
          # å¤„ç†æ¯ä¸ªå—
          processed_chunk = process_chunk(chunk)
          results.append(processed_chunk)
      
      return pd.concat(results, ignore_index=True)
  ```

  ### å¹¶è¡Œå¤„ç†
  ```python
  from multiprocessing import Pool
  import dask.dataframe as dd
  
  # ä½¿ç”¨Daskè¿›è¡Œå¹¶è¡Œå¤„ç†
  def parallel_processing_with_dask(df):
      """ä½¿ç”¨Daskè¿›è¡Œå¹¶è¡Œæ•°æ®å¤„ç†"""
      # è½¬æ¢ä¸ºDask DataFrame
      ddf = dd.from_pandas(df, npartitions=4)
      
      # å¹¶è¡Œè®¡ç®—
      result = ddf.groupby('category').value.mean().compute()
      
      return result
  
  # ä½¿ç”¨multiprocessing
  def parallel_apply(df, func, n_cores=4):
      """å¹¶è¡Œåº”ç”¨å‡½æ•°"""
      df_split = np.array_split(df, n_cores)
      
      with Pool(n_cores) as pool:
          results = pool.map(func, df_split)
      
      return pd.concat(results)
  ```
</knowledge>
