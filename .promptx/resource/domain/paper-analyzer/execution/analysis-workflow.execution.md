<execution>
  <constraint>
    ## è®ºæ–‡åˆ†æé™åˆ¶æ¡ä»¶
    - **æ ¼å¼çº¦æŸ**ï¼šä¸»è¦æ”¯æŒPDFæ ¼å¼ï¼Œéƒ¨åˆ†æ”¯æŒå…¶ä»–æ–‡æ¡£æ ¼å¼
    - **è¯­è¨€çº¦æŸ**ï¼šä¸»è¦æ”¯æŒä¸­è‹±æ–‡ï¼Œå…¶ä»–è¯­è¨€æ”¯æŒæœ‰é™
    - **å¤§å°çº¦æŸ**ï¼šå•ä¸ªæ–‡ä»¶å¤§å°ä¸è¶…è¿‡100MB
    - **è´¨é‡çº¦æŸ**ï¼šPDFè´¨é‡å½±å“æ–‡æœ¬æå–å‡†ç¡®æ€§
    - **ç‰ˆæƒçº¦æŸ**ï¼šå¿…é¡»éµå®ˆå­¦æœ¯è®ºæ–‡çš„ç‰ˆæƒå’Œä½¿ç”¨è§„å®š
  </constraint>

  <rule>
    ## è®ºæ–‡åˆ†æå¼ºåˆ¶è§„åˆ™
    - **å®Œæ•´æ€§æ£€æŸ¥**ï¼šåˆ†æå‰å¿…é¡»éªŒè¯æ–‡ä»¶å®Œæ•´æ€§
    - **ç»“æ„è¯†åˆ«**ï¼šå¿…é¡»æ­£ç¡®è¯†åˆ«è®ºæ–‡çš„åŸºæœ¬ç»“æ„
    - **å¼•ç”¨ä¿ç•™**ï¼šå¿…é¡»ä¿ç•™åŸæ–‡çš„å¼•ç”¨å’Œå‚è€ƒä¿¡æ¯
    - **è´¨é‡æ ‡è®°**ï¼šå¿…é¡»æ ‡è®°æå–å†…å®¹çš„å¯ä¿¡åº¦
    - **ç‰ˆæƒå£°æ˜**ï¼šå¿…é¡»ä¿ç•™åŸæ–‡çš„ç‰ˆæƒä¿¡æ¯
  </rule>

  <guideline>
    ## è®ºæ–‡åˆ†ææŒ‡å¯¼åŸåˆ™
    - **å‡†ç¡®æ€§ä¼˜å…ˆ**ï¼šç¡®ä¿æå–ä¿¡æ¯çš„å‡†ç¡®æ€§
    - **ç»“æ„åŒ–è¾“å‡º**ï¼šæä¾›ç»“æ„åŒ–çš„åˆ†æç»“æœ
    - **å¤šå±‚æ¬¡åˆ†æ**ï¼šä»æ¦‚è§ˆåˆ°ç»†èŠ‚çš„å¤šå±‚æ¬¡åˆ†æ
    - **ä¸Šä¸‹æ–‡ä¿æŒ**ï¼šä¿æŒä¿¡æ¯çš„ä¸Šä¸‹æ–‡å…³ç³»
    - **å¯è¿½æº¯æ€§**ï¼šç¡®ä¿åˆ†æç»“æœå¯è¿½æº¯åˆ°åŸæ–‡
  </guideline>

  <process>
    ## ğŸ“„ è®ºæ–‡åˆ†æå·¥ä½œæµç¨‹

    ### è®ºæ–‡åˆ†ææ¶æ„
    ```mermaid
    graph TD
        A[PDFè¾“å…¥] --> B[æ–‡æ¡£é¢„å¤„ç†]
        B --> C[ç»“æ„è¯†åˆ«]
        C --> D[å†…å®¹æå–]
        D --> E[ä¿¡æ¯åˆ†æ]
        E --> F[çŸ¥è¯†æŠ½å–]
        F --> G[ç»“æœè¾“å‡º]
        
        B --> B1[æ ¼å¼éªŒè¯]
        B --> B2[OCRå¤„ç†]
        B --> B3[æ–‡æœ¬æ¸…æ´—]
        
        C --> C1[æ ‡é¢˜è¯†åˆ«]
        C --> C2[ç« èŠ‚åˆ’åˆ†]
        C --> C3[å›¾è¡¨å®šä½]
        
        D --> D1[æ‘˜è¦æå–]
        D --> D2[å…³é”®è¯æå–]
        D --> D3[æ­£æ–‡æå–]
        D --> D4[å‚è€ƒæ–‡çŒ®æå–]
        
        E --> E1[æ¦‚å¿µè¯†åˆ«]
        E --> E2[æ–¹æ³•åˆ†æ]
        E --> E3[ç»“æœè¯„ä¼°]
    ```

    ### ç¬¬ä¸€é˜¶æ®µï¼šæ–‡æ¡£é¢„å¤„ç†
    ```mermaid
    flowchart TD
        A[PDFæ–‡ä»¶] --> B[æ–‡ä»¶éªŒè¯]
        B --> C[æ ¼å¼æ£€æŸ¥]
        C --> D[æ–‡æœ¬æå–]
        D --> E[OCRå¤„ç†]
        E --> F[æ–‡æœ¬æ¸…æ´—]
        F --> G[ç¼–ç æ ‡å‡†åŒ–]
        
        B --> B1[æ–‡ä»¶å®Œæ•´æ€§]
        B --> B2[æ–‡ä»¶å¤§å°]
        B --> B3[æƒé™æ£€æŸ¥]
        
        C --> C1[PDFç‰ˆæœ¬]
        C --> C2[é¡µé¢æ•°é‡]
        C --> C3[å›¾æ–‡æ··åˆ]
        
        D --> D1[ç›´æ¥æ–‡æœ¬æå–]
        D --> D2[è¡¨æ ¼æå–]
        D --> D3[å›¾åƒè¯†åˆ«]
        
        E --> E1[å›¾åƒæ–‡å­—è¯†åˆ«]
        E --> E2[æ‰‹å†™æ–‡å­—è¯†åˆ«]
        E --> E3[å…¬å¼è¯†åˆ«]
    ```

    ### ç¬¬äºŒé˜¶æ®µï¼šç»“æ„åŒ–åˆ†æ
    ```mermaid
    graph TD
        A[é¢„å¤„ç†æ–‡æœ¬] --> B[æ–‡æ¡£ç»“æ„åˆ†æ]
        B --> C[å†…å®¹åˆ†ç±»]
        C --> D[å…³é”®ä¿¡æ¯æå–]
        D --> E[å…³ç³»è¯†åˆ«]
        E --> F[è´¨é‡è¯„ä¼°]
        
        B --> B1[æ ‡é¢˜å±‚æ¬¡]
        B --> B2[æ®µè½ç»“æ„]
        B --> B3[åˆ—è¡¨è¯†åˆ«]
        
        C --> C1[æ‘˜è¦]
        C --> C2[å¼•è¨€]
        C --> C3[æ–¹æ³•]
        C --> C4[ç»“æœ]
        C --> C5[è®¨è®º]
        C --> C6[ç»“è®º]
        
        D --> D1[ç ”ç©¶é—®é¢˜]
        D --> D2[ç ”ç©¶æ–¹æ³•]
        D --> D3[ä¸»è¦å‘ç°]
        D --> D4[åˆ›æ–°ç‚¹]
        
        E --> E1[æ¦‚å¿µå…³ç³»]
        E --> E2[å› æœå…³ç³»]
        E --> E3[å¼•ç”¨å…³ç³»]
    ```

    ### ç¬¬ä¸‰é˜¶æ®µï¼šçŸ¥è¯†æŠ½å–ä¸åˆ†æ
    ```mermaid
    flowchart LR
        A[ç»“æ„åŒ–å†…å®¹] --> B[æ¦‚å¿µæŠ½å–]
        B --> C[æ–¹æ³•è¯†åˆ«]
        C --> D[ç»“æœåˆ†æ]
        D --> E[ä»·å€¼è¯„ä¼°]
        E --> F[çŸ¥è¯†å›¾è°±]
        F --> G[åº”ç”¨å»ºè®®]
        
        B --> B1[æ ¸å¿ƒæ¦‚å¿µ]
        B --> B2[æŠ€æœ¯æœ¯è¯­]
        B --> B3[ç†è®ºæ¡†æ¶]
        
        C --> C1[ç ”ç©¶æ–¹æ³•]
        C --> C2[å®éªŒè®¾è®¡]
        C --> C3[æ•°æ®åˆ†æ]
        
        D --> D1[å®šé‡ç»“æœ]
        D --> D2[å®šæ€§å‘ç°]
        D --> D3[ç»Ÿè®¡æ˜¾è‘—æ€§]
        
        E --> E1[å­¦æœ¯ä»·å€¼]
        E --> E2[å®ç”¨ä»·å€¼]
        E --> E3[åˆ›æ–°ç¨‹åº¦]
    ```

    ## ğŸ› ï¸ è®ºæ–‡åˆ†ææŠ€æœ¯å®ç°

    ### PDFå¤„ç†å¼•æ“
    ```python
    import PyPDF2
    import pdfplumber
    import fitz  # PyMuPDF
    from PIL import Image
    import pytesseract
    import re
    from typing import Dict, List, Any, Optional
    import logging

    class PDFProcessor:
        def __init__(self):
            self.logger = logging.getLogger(__name__)
        
        def extract_text_pypdf2(self, pdf_path: str) -> str:
            """ä½¿ç”¨PyPDF2æå–æ–‡æœ¬"""
            text = ""
            try:
                with open(pdf_path, 'rb') as file:
                    pdf_reader = PyPDF2.PdfReader(file)
                    for page in pdf_reader.pages:
                        text += page.extract_text() + "\n"
            except Exception as e:
                self.logger.error(f"PyPDF2 extraction failed: {e}")
            return text
        
        def extract_text_pdfplumber(self, pdf_path: str) -> Dict[str, Any]:
            """ä½¿ç”¨pdfplumberæå–æ–‡æœ¬å’Œè¡¨æ ¼"""
            result = {"text": "", "tables": [], "metadata": {}}
            
            try:
                with pdfplumber.open(pdf_path) as pdf:
                    result["metadata"] = pdf.metadata
                    
                    for page_num, page in enumerate(pdf.pages):
                        # æå–æ–‡æœ¬
                        page_text = page.extract_text()
                        if page_text:
                            result["text"] += f"\n--- Page {page_num + 1} ---\n{page_text}\n"
                        
                        # æå–è¡¨æ ¼
                        tables = page.extract_tables()
                        for table in tables:
                            result["tables"].append({
                                "page": page_num + 1,
                                "data": table
                            })
            
            except Exception as e:
                self.logger.error(f"pdfplumber extraction failed: {e}")
            
            return result
        
        def extract_text_pymupdf(self, pdf_path: str) -> Dict[str, Any]:
            """ä½¿ç”¨PyMuPDFæå–æ–‡æœ¬å’Œå›¾åƒ"""
            result = {"text": "", "images": [], "metadata": {}}
            
            try:
                doc = fitz.open(pdf_path)
                result["metadata"] = doc.metadata
                
                for page_num in range(len(doc)):
                    page = doc.load_page(page_num)
                    
                    # æå–æ–‡æœ¬
                    text = page.get_text()
                    result["text"] += f"\n--- Page {page_num + 1} ---\n{text}\n"
                    
                    # æå–å›¾åƒ
                    image_list = page.get_images()
                    for img_index, img in enumerate(image_list):
                        xref = img[0]
                        pix = fitz.Pixmap(doc, xref)
                        
                        if pix.n - pix.alpha < 4:  # GRAY or RGB
                            img_data = pix.tobytes("png")
                            result["images"].append({
                                "page": page_num + 1,
                                "index": img_index,
                                "data": img_data
                            })
                        pix = None
                
                doc.close()
            
            except Exception as e:
                self.logger.error(f"PyMuPDF extraction failed: {e}")
            
            return result
        
        def ocr_image(self, image_data: bytes) -> str:
            """OCRå›¾åƒè¯†åˆ«"""
            try:
                image = Image.open(io.BytesIO(image_data))
                text = pytesseract.image_to_string(image, lang='chi_sim+eng')
                return text
            except Exception as e:
                self.logger.error(f"OCR failed: {e}")
                return ""
        
        def extract_comprehensive(self, pdf_path: str) -> Dict[str, Any]:
            """ç»¼åˆæå–æ–¹æ³•"""
            result = {
                "text": "",
                "tables": [],
                "images": [],
                "metadata": {},
                "extraction_methods": []
            }
            
            # å°è¯•å¤šç§æå–æ–¹æ³•
            methods = [
                ("pdfplumber", self.extract_text_pdfplumber),
                ("pymupdf", self.extract_text_pymupdf),
                ("pypdf2", self.extract_text_pypdf2)
            ]
            
            for method_name, method_func in methods:
                try:
                    if method_name == "pypdf2":
                        text = method_func(pdf_path)
                        if text and len(text.strip()) > 100:
                            result["text"] = text
                            result["extraction_methods"].append(method_name)
                            break
                    else:
                        extracted = method_func(pdf_path)
                        if extracted["text"] and len(extracted["text"].strip()) > 100:
                            result.update(extracted)
                            result["extraction_methods"].append(method_name)
                            break
                
                except Exception as e:
                    self.logger.warning(f"Method {method_name} failed: {e}")
                    continue
            
            return result
    ```

    ### å­¦æœ¯è®ºæ–‡ç»“æ„è¯†åˆ«
    ```python
    import re
    from dataclasses import dataclass
    from typing import List, Dict, Optional

    @dataclass
    class PaperSection:
        title: str
        content: str
        level: int
        start_pos: int
        end_pos: int

    class PaperStructureAnalyzer:
        def __init__(self):
            self.section_patterns = [
                r'(?i)^(abstract|æ‘˜è¦)',
                r'(?i)^(introduction|å¼•è¨€|1\.?\s*introduction)',
                r'(?i)^(related work|ç›¸å…³å·¥ä½œ|literature review|æ–‡çŒ®ç»¼è¿°)',
                r'(?i)^(method|methodology|æ–¹æ³•|ç®—æ³•)',
                r'(?i)^(experiment|å®éªŒ|evaluation|è¯„ä¼°)',
                r'(?i)^(result|ç»“æœ|findings|å‘ç°)',
                r'(?i)^(discussion|è®¨è®º|analysis|åˆ†æ)',
                r'(?i)^(conclusion|ç»“è®º|summary|æ€»ç»“)',
                r'(?i)^(reference|å‚è€ƒæ–‡çŒ®|bibliography)'
            ]
        
        def identify_sections(self, text: str) -> List[PaperSection]:
            """è¯†åˆ«è®ºæ–‡ç« èŠ‚ç»“æ„"""
            sections = []
            lines = text.split('\n')
            
            current_section = None
            content_buffer = []
            
            for i, line in enumerate(lines):
                line = line.strip()
                if not line:
                    continue
                
                # æ£€æŸ¥æ˜¯å¦æ˜¯ç« èŠ‚æ ‡é¢˜
                section_match = self._is_section_title(line)
                if section_match:
                    # ä¿å­˜å‰ä¸€ä¸ªç« èŠ‚
                    if current_section:
                        current_section.content = '\n'.join(content_buffer)
                        current_section.end_pos = i
                        sections.append(current_section)
                    
                    # å¼€å§‹æ–°ç« èŠ‚
                    current_section = PaperSection(
                        title=line,
                        content="",
                        level=self._get_section_level(line),
                        start_pos=i,
                        end_pos=0
                    )
                    content_buffer = []
                else:
                    content_buffer.append(line)
            
            # ä¿å­˜æœ€åä¸€ä¸ªç« èŠ‚
            if current_section:
                current_section.content = '\n'.join(content_buffer)
                current_section.end_pos = len(lines)
                sections.append(current_section)
            
            return sections
        
        def _is_section_title(self, line: str) -> bool:
            """åˆ¤æ–­æ˜¯å¦æ˜¯ç« èŠ‚æ ‡é¢˜"""
            # æ£€æŸ¥ç¼–å·æ¨¡å¼
            if re.match(r'^\d+\.?\s+[A-Za-z\u4e00-\u9fff]', line):
                return True
            
            # æ£€æŸ¥å…³é”®è¯æ¨¡å¼
            for pattern in self.section_patterns:
                if re.match(pattern, line):
                    return True
            
            # æ£€æŸ¥æ ¼å¼ç‰¹å¾ï¼ˆå…¨å¤§å†™ã€å±…ä¸­ç­‰ï¼‰
            if line.isupper() and len(line.split()) <= 5:
                return True
            
            return False
        
        def _get_section_level(self, title: str) -> int:
            """è·å–ç« èŠ‚å±‚çº§"""
            # åŸºäºç¼–å·åˆ¤æ–­å±‚çº§
            match = re.match(r'^(\d+)\.', title)
            if match:
                return len(match.group(1))
            
            # åŸºäºæ ¼å¼åˆ¤æ–­å±‚çº§
            if title.isupper():
                return 1
            elif title.istitle():
                return 2
            else:
                return 3
        
        def extract_abstract(self, sections: List[PaperSection]) -> Optional[str]:
            """æå–æ‘˜è¦"""
            for section in sections:
                if re.match(r'(?i)(abstract|æ‘˜è¦)', section.title):
                    return section.content
            return None
        
        def extract_keywords(self, text: str) -> List[str]:
            """æå–å…³é”®è¯"""
            keywords = []
            
            # æŸ¥æ‰¾å…³é”®è¯éƒ¨åˆ†
            keyword_patterns = [
                r'(?i)keywords?[:\s]+(.*?)(?:\n|$)',
                r'(?i)å…³é”®è¯[:\s]+(.*?)(?:\n|$)',
                r'(?i)key words?[:\s]+(.*?)(?:\n|$)'
            ]
            
            for pattern in keyword_patterns:
                matches = re.findall(pattern, text)
                for match in matches:
                    # åˆ†å‰²å…³é”®è¯
                    words = re.split(r'[,;ï¼Œï¼›]', match)
                    keywords.extend([word.strip() for word in words if word.strip()])
            
            return keywords
    ```

    ### å†…å®¹åˆ†æå¼•æ“
    ```python
    import spacy
    from collections import Counter
    from typing import Set

    class ContentAnalyzer:
        def __init__(self):
            try:
                self.nlp = spacy.load("en_core_web_sm")
            except OSError:
                self.nlp = None
                logging.warning("spaCy English model not found")
        
        def extract_entities(self, text: str) -> Dict[str, List[str]]:
            """æå–å‘½åå®ä½“"""
            if not self.nlp:
                return {}
            
            doc = self.nlp(text)
            entities = {}
            
            for ent in doc.ents:
                if ent.label_ not in entities:
                    entities[ent.label_] = []
                entities[ent.label_].append(ent.text)
            
            # å»é‡å¹¶æ’åº
            for label in entities:
                entities[label] = list(set(entities[label]))
            
            return entities
        
        def extract_key_phrases(self, text: str, top_k: int = 20) -> List[str]:
            """æå–å…³é”®çŸ­è¯­"""
            if not self.nlp:
                return []
            
            doc = self.nlp(text)
            
            # æå–åè¯çŸ­è¯­
            noun_phrases = []
            for chunk in doc.noun_chunks:
                if len(chunk.text.split()) >= 2:  # è‡³å°‘ä¸¤ä¸ªè¯
                    noun_phrases.append(chunk.text.lower())
            
            # ç»Ÿè®¡é¢‘ç‡
            phrase_counts = Counter(noun_phrases)
            
            # è¿”å›æœ€é¢‘ç¹çš„çŸ­è¯­
            return [phrase for phrase, count in phrase_counts.most_common(top_k)]
        
        def analyze_methodology(self, text: str) -> Dict[str, Any]:
            """åˆ†æç ”ç©¶æ–¹æ³•"""
            method_keywords = {
                'quantitative': ['survey', 'experiment', 'statistical', 'regression', 'correlation'],
                'qualitative': ['interview', 'case study', 'ethnography', 'grounded theory'],
                'mixed_methods': ['mixed method', 'triangulation', 'sequential', 'concurrent'],
                'machine_learning': ['neural network', 'deep learning', 'classification', 'clustering'],
                'statistical': ['t-test', 'anova', 'chi-square', 'regression', 'significance']
            }
            
            text_lower = text.lower()
            detected_methods = {}
            
            for method_type, keywords in method_keywords.items():
                count = sum(text_lower.count(keyword) for keyword in keywords)
                if count > 0:
                    detected_methods[method_type] = count
            
            return detected_methods
        
        def assess_paper_quality(self, sections: List[PaperSection], 
                               metadata: Dict[str, Any]) -> Dict[str, Any]:
            """è¯„ä¼°è®ºæ–‡è´¨é‡"""
            quality_metrics = {
                'structure_completeness': 0,
                'content_depth': 0,
                'methodology_clarity': 0,
                'citation_quality': 0,
                'overall_score': 0
            }
            
            # ç»“æ„å®Œæ•´æ€§è¯„ä¼°
            required_sections = ['abstract', 'introduction', 'method', 'result', 'conclusion']
            found_sections = []
            
            for section in sections:
                title_lower = section.title.lower()
                for req_section in required_sections:
                    if req_section in title_lower:
                        found_sections.append(req_section)
                        break
            
            quality_metrics['structure_completeness'] = len(set(found_sections)) / len(required_sections)
            
            # å†…å®¹æ·±åº¦è¯„ä¼°ï¼ˆåŸºäºé•¿åº¦å’Œå¤æ‚æ€§ï¼‰
            total_length = sum(len(section.content) for section in sections)
            if total_length > 10000:  # å‡è®¾é«˜è´¨é‡è®ºæ–‡è‡³å°‘10kå­—ç¬¦
                quality_metrics['content_depth'] = min(total_length / 20000, 1.0)
            
            # æ–¹æ³•è®ºæ¸…æ™°åº¦ï¼ˆåŸºäºæ–¹æ³•å…³é”®è¯ï¼‰
            method_section = next((s for s in sections if 'method' in s.title.lower()), None)
            if method_section:
                method_analysis = self.analyze_methodology(method_section.content)
                quality_metrics['methodology_clarity'] = min(len(method_analysis) / 3, 1.0)
            
            # å¼•ç”¨è´¨é‡ï¼ˆåŸºäºå‚è€ƒæ–‡çŒ®æ•°é‡ï¼‰
            ref_section = next((s for s in sections if 'reference' in s.title.lower()), None)
            if ref_section:
                ref_count = len(re.findall(r'\[\d+\]', ref_section.content))
                quality_metrics['citation_quality'] = min(ref_count / 50, 1.0)
            
            # è®¡ç®—æ€»åˆ†
            quality_metrics['overall_score'] = sum(quality_metrics.values()) / 4
            
            return quality_metrics
    ```

    ## ğŸ“Š åˆ†æç»“æœè¾“å‡º

    ### ç»“æ„åŒ–æŠ¥å‘Šç”Ÿæˆ
    ```python
    class PaperAnalysisReport:
        def __init__(self, paper_path: str):
            self.paper_path = paper_path
            self.processor = PDFProcessor()
            self.structure_analyzer = PaperStructureAnalyzer()
            self.content_analyzer = ContentAnalyzer()
        
        def generate_full_report(self) -> Dict[str, Any]:
            """ç”Ÿæˆå®Œæ•´åˆ†ææŠ¥å‘Š"""
            # 1. æå–æ–‡æ¡£å†…å®¹
            extracted_data = self.processor.extract_comprehensive(self.paper_path)
            
            # 2. åˆ†ææ–‡æ¡£ç»“æ„
            sections = self.structure_analyzer.identify_sections(extracted_data["text"])
            
            # 3. æå–å…³é”®ä¿¡æ¯
            abstract = self.structure_analyzer.extract_abstract(sections)
            keywords = self.structure_analyzer.extract_keywords(extracted_data["text"])
            
            # 4. å†…å®¹åˆ†æ
            entities = self.content_analyzer.extract_entities(extracted_data["text"])
            key_phrases = self.content_analyzer.extract_key_phrases(extracted_data["text"])
            methodology = self.content_analyzer.analyze_methodology(extracted_data["text"])
            
            # 5. è´¨é‡è¯„ä¼°
            quality_assessment = self.content_analyzer.assess_paper_quality(
                sections, extracted_data["metadata"]
            )
            
            # 6. ç”ŸæˆæŠ¥å‘Š
            report = {
                "paper_info": {
                    "file_path": self.paper_path,
                    "metadata": extracted_data["metadata"],
                    "extraction_methods": extracted_data["extraction_methods"]
                },
                "structure": {
                    "sections": [
                        {
                            "title": section.title,
                            "level": section.level,
                            "word_count": len(section.content.split())
                        } for section in sections
                    ],
                    "total_sections": len(sections)
                },
                "content": {
                    "abstract": abstract,
                    "keywords": keywords,
                    "key_phrases": key_phrases,
                    "entities": entities,
                    "methodology": methodology
                },
                "quality": quality_assessment,
                "analysis_timestamp": datetime.now().isoformat()
            }
            
            return report
        
        def generate_summary(self, report: Dict[str, Any]) -> str:
            """ç”Ÿæˆåˆ†ææ‘˜è¦"""
            summary = f"""
            è®ºæ–‡åˆ†ææŠ¥å‘Šæ‘˜è¦
            ==================
            
            æ–‡ä»¶: {report['paper_info']['file_path']}
            åˆ†ææ—¶é—´: {report['analysis_timestamp']}
            
            ç»“æ„åˆ†æ:
            - ç« èŠ‚æ•°é‡: {report['structure']['total_sections']}
            - ä¸»è¦ç« èŠ‚: {', '.join([s['title'] for s in report['structure']['sections'][:5]])}
            
            å†…å®¹åˆ†æ:
            - å…³é”®è¯: {', '.join(report['content']['keywords'][:10])}
            - ç ”ç©¶æ–¹æ³•: {', '.join(report['content']['methodology'].keys())}
            
            è´¨é‡è¯„ä¼°:
            - ç»“æ„å®Œæ•´æ€§: {report['quality']['structure_completeness']:.2%}
            - å†…å®¹æ·±åº¦: {report['quality']['content_depth']:.2%}
            - æ€»ä½“è¯„åˆ†: {report['quality']['overall_score']:.2%}
            """
            
            return summary
    ```
  </process>

  <criteria>
    ## è®ºæ–‡åˆ†æè¯„ä»·æ ‡å‡†

    ### æå–å‡†ç¡®æ€§
    - âœ… æ–‡æœ¬æå–å‡†ç¡®ç‡ â‰¥ 95%
    - âœ… ç»“æ„è¯†åˆ«å‡†ç¡®ç‡ â‰¥ 90%
    - âœ… å…³é”®ä¿¡æ¯æå–å®Œæ•´æ€§ â‰¥ 85%
    - âœ… è¡¨æ ¼å›¾åƒè¯†åˆ«ç‡ â‰¥ 80%

    ### åˆ†ææ·±åº¦
    - âœ… æ¦‚å¿µè¯†åˆ«è¦†ç›–ç‡ â‰¥ 90%
    - âœ… æ–¹æ³•è®ºåˆ†æå‡†ç¡®æ€§ â‰¥ 85%
    - âœ… è´¨é‡è¯„ä¼°åˆç†æ€§ â‰¥ 80%
    - âœ… çŸ¥è¯†æŠ½å–æœ‰æ•ˆæ€§ â‰¥ 85%

    ### å¤„ç†æ•ˆç‡
    - âœ… å•ç¯‡è®ºæ–‡å¤„ç†æ—¶é—´ â‰¤ 5åˆ†é’Ÿ
    - âœ… æ‰¹é‡å¤„ç†èƒ½åŠ› â‰¥ 10ç¯‡/å°æ—¶
    - âœ… å†…å­˜ä½¿ç”¨åˆç†
    - âœ… é”™è¯¯æ¢å¤èƒ½åŠ›å¼º

    ### è¾“å‡ºè´¨é‡
    - âœ… æŠ¥å‘Šç»“æ„æ¸…æ™°
    - âœ… ä¿¡æ¯ç»„ç»‡åˆç†
    - âœ… å¯è¯»æ€§å¼º
    - âœ… å¯æ“ä½œæ€§é«˜
  </criteria>
</execution>
